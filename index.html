---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: default
---

<style>
    .video-container {
        position: relative;
        padding-bottom: 56.25%; /* 16:9 aspect ratio */
        height: 0;
        overflow: hidden;
        max-width: 100%;
        background: #000;
    }
    .video-container iframe {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }
    audio {
       width: 100%;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
   .table-container {
        width: 100%;
        max-width: 100%;
        overflow-x: auto;
        -webkit-overflow-scrolling: touch; /* For smooth scrolling on mobile devices */
    }
    html, body {
        margin: 5px; /* Get rid of default margins */
        padding: 5px; /* Get rid of default padding */
        overflow-x: hidden; /* Prevent horizontal overflow */
    }
    th, td {
        padding: 4px;
        text-align: left;
        border: 1px solid #ddd;
        font-size: 1em; /* Base font size */
        white-space: nowrap;
    }
    p {
        text-align: justify;
        hyphens: auto;
    }   
    div {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        max-width: 100%;
    }
    /* Define a class for left-aligned text */
    .left-align {
        text-align: left;
        hyphens: none;
    } 
    .container {
        max-width: 100%;
        background-color: #f4f4f4;
        align-items: left;
        overflow-x: auto; /* Handle overflow */
        padding: 0;
        margin: 0 auto;
        position: relative; 
    }
    .inner-container {
        width: calc(100% - 30px);
        background-color: #f4f4f4;
        align-items: left;
        padding: 0;
        margin: 0 auto;
        overflow: hidden;
        position: relative; /* Relative positioning for absolute child positioning */
    }
    pre {
        width: calc(100% - 30px);
        background-color: #f4f4f4;
        border: 0;
        padding: 0px;
        overflow: auto;
        border-radius: 0px;
        margin: 0;
    }
    code {
        font-family: monospace;
        padding: 0px;
        margin: 0;
    }
    .highlight {
        cursor: pointer;
        padding: 4px 4px;
        background-color: #f4f4f4;
        border: 0;
        color: white;
        border-radius: 4px;
        display: flex;
        align-items: center;
        max-width: 100%;
        position: absolute;
        top: 0; /* Position the button at the top */
        right: 0; /* Position the button at the right edge */
        margin: 5px; /* Small margin for aesthetics */
    }
    
    .highlight svg {
        fill: #4e4e4e;
        margin-right: 5px;
    }

    .highlight:hover {
        background-color: #d3d3d3;
    }
</style>

<h1>
    Enhancing In-the-Wild Speech Emotion Conversion with Resynthesis-based Duration Modeling
</h1>

<p>
    Speech Emotion Conversion aims to modify the emotion expressed in input speech while preserving lexical content and speaker identity. Recently, generative modeling approaches have shown promising results in changing local acoustic properties such as fundamental frequency, spectral envelope and energy, but often lack the ability to control the duration of sounds. To address this, we propose a duration modeling framework using resynthesis-based discrete content representations, enabling modification of speech duration to reflect target emotions and achieve controllable speech rates without using parallel data. Experimental results reveal that the inclusion of the proposed duration modeling framework significantly enhances emotional expressiveness, in the in-the-wild MSP-Podcast dataset. Analyses show that low-arousal emotions correlate with longer durations and slower speech rates, while high-arousal emotions produce shorter, faster speech. 
</p>

<h2>
    Audio Examples
</h2>

<!-- <p>
    p002/emo_adoration_sentences.wav<br>
    <audio controls>
        <source src="https://www2.informatik.uni-hamburg.de/sp/audio/publications/interspeech2024-ears/files/ears/p002_emo_adoration_sentences.wav" type="audio/wav">
        Your browser does not support the audio element.
    </audio>
    <br>

    p008/emo_contentment_sentences.wav<br>
    <audio controls>
        <source src="https://www2.informatik.uni-hamburg.de/sp/audio/publications/interspeech2024-ears/files/ears/p008_emo_contentment_sentences.wav" type="audio/wav">
        Your browser does not support the audio element.
        </audio>
    <br>

    p010/emo_cuteness_sentences.wav<br>
    <audio controls>
        <source src="https://www2.informatik.uni-hamburg.de/sp/audio/publications/interspeech2024-ears/files/ears/p010_emo_cuteness_sentences.wav" type="audio/wav">
        Your browser does not support the audio element.
    </audio>
    <br>
</p> -->

<!-- <h3>
    With dropdown menu 
</h3> -->

<p>Select an audio file: &nbsp;&nbsp;
    <select id="audioSelect" onchange="playAudio()">
        <option value="msp_podcast_0003_0442">msp_podcast_0003_0442</option>
    <option value="msp_podcast_0202_0002">msp_podcast_0202_0002</option>
    <option value="msp_podcast_0538_0133">msp_podcast_0538_0133</option>
    <option value="msp_podcast_2329_1765">msp_podcast_2329_1765</option>
    <!-- <option value="p106/00049_5.4dB">p106/00049_5.4dB</option>
    <option value="p107/00519_8.1dB">p107/00519_8.1dB</option> -->
    </select>
</p>

<!-- https://www2.informatik.uni-hamburg.de/sp/audio/publications/asru2025_durmodel/files/msppod/durmodel/gt/msp_podcast_0003_0442.wav -->
<!-- Below is for default Audio Sample Selection setup -->
<p>
    Ground-truth Audio: <br>
    <audio id="audioPlayerGT" controls>
        <source id="audioSourceGT" src="https://www2.informatik.uni-hamburg.de/sp/audio/publications/asru2025_durmodel/files/msppod/durmodel/gt/msp_podcast_0003_0442.wav" type="audio/wav">
        Your browser does not support the audio element.
    </audio><br>

    <h2>
        Emotion Converted Speech
    </h2>

    Low Arousal: 1 <span class="reference" data-ref="low-arousal"></span>:<br>
    <audio id="audioPlayerLowArousal" controls>
        <source id="audioSourceLowArousal" src="https://www2.informatik.uni-hamburg.de/sp/audio/publications/asru2025_durmodel/files/msppod/durmodel/arous1/msp_podcast_0003_0442.wav" type="audio/wav">
        Your browser does not support the audio element.
    </audio><br>

    Low Arousal: 4 <span class="reference" data-ref="med-arousal"></span>:<br>
    <audio id="audioPlayerMedArousal" controls>
        <source id="audioSourceMedArousal" src="https://www2.informatik.uni-hamburg.de/sp/audio/publications/asru2025_durmodel/files/msppod/durmodel/arous4/msp_podcast_0003_0442.wav" type="audio/wav">
        Your browser does not support the audio element.
    </audio><br>

    Low Arousal: 7 <span class="reference" data-ref="high-arousal"></span>:<br>
    <audio id="audioPlayerHighArousal" controls>
        <source id="audioSourceHighArousal" src="https://www2.informatik.uni-hamburg.de/sp/audio/publications/asru2025_durmodel/files/msppod/durmodel/arous7/msp_podcast_0003_0442.wav" type="audio/wav">
        Your browser does not support the audio element.
    </audio>
</p>

<script>
    function playAudio() {
        var audioPlayerGT = document.getElementById('audioPlayerGT');
        var audioPlayerLowArousal = document.getElementById('audioPlayerLowArousal');
        var audioPlayerMedArousal = document.getElementById('audioPlayerMedArousal');
        var audioPlayerHighArousal = document.getElementById('audioPlayerHighArousal');

        var audioSourceGT = document.getElementById('audioSourceGT');
        var audioSourceLowArousal = document.getElementById('audioSourceLowArousal');
        var audioSourceMedArousal = document.getElementById('audioSourceMedArousal');
        var audioSourceHighArousal = document.getElementById('audioSourceHighArousal');

        var selectedAudio = document.getElementById('audioSelect').value;

        if (selectedAudio) {
            // Set the source of the source elements, not the audio players
            audioSourceGT.src = "https://www2.informatik.uni-hamburg.de/sp/audio/publications/asru2025_durmodel/files/msppod/durmodel/gt/" + selectedAudio + ".wav";
            audioPlayerGT.load(); // Load the selected audio file using the audio player

            audioSourceLowArousal.src = "https://www2.informatik.uni-hamburg.de/sp/audio/publications/asru2025_durmodel/files/msppod/durmodel/arous1/" + selectedAudio + ".wav";
            audioPlayerLowArousal.load(); // Load the selected audio file using the audio player

            audioSourceMedArousal.src = "https://www2.informatik.uni-hamburg.de/sp/audio/publications/asru2025_durmodel/files/msppod/durmodel/arous4/" + selectedAudio + ".wav";
            audioPlayerMedArousal.load(); // Load the selected audio file using the audio player

            audioSourceHighArousal.src = "https://www2.informatik.uni-hamburg.de/sp/audio/publications/asru2025_durmodel/files/msppod/durmodel/arous7/" + selectedAudio + ".wav";
            audioPlayerHighArousal.load(); // Load the selected audio file using the audio player

            // This will ensure that the audio players are updating their children source elements
            // and reloading them
        }
    }
</script>
<br>

<!-- <h2>
    Youtube embedding 
</h2>

<div class="video-container">
    <iframe src="https://www.youtube.com/embed/H5FiO0JxPK4" frameborder="0" allowfullscreen></iframe>
</div> -->

<!-- <br> -->

<h2 id="citation">
    Citation
</h2>

<div class="container">
    <div class="inner-container" dir="auto" data-snippet-clipboard-copy-content="{% raw %}@inproceedings{rajprabhu2025durmodel,
    title={Enhancing In-the-Wild Speech Emotion Conversion with Resynthesis-based Duration Modeling},
    author={Raj Prabhu, Navin and de Oliveira, Danilo and Lehmann-Willenbrock, Nale and Gerkmann, Timo},
    booktitle={IEEE ASRU},
    year={2025}
}{% endraw %}">
<pre><code>{% raw %}@inproceedings{rajprabhu2025durmodel,
    title={Enhancing In-the-Wild Speech Emotion Conversion with Resynthesis-based Duration Modeling},
    author={Raj Prabhu, Navin and de Oliveira, Danilo and Lehmann-Willenbrock, Nale and Gerkmann, Timo},
    booktitle={IEEE ASRU},
    year={2025}
}{% endraw %}</code></pre>
    </div>
    <button id="copyButton" class="highlight">
        <svg xmlns="http://www.w3.org/2000/svg" height="16" viewBox="0 0 16 16" width="16" class="octicon octicon-clippy">
            <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path>
            <path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
        </svg>
    </button>
</div>

<script>
    document.getElementById('copyButton').addEventListener('click', function() {
        const codeContent = document.querySelector('div[data-snippet-clipboard-copy-content]').getAttribute('data-snippet-clipboard-copy-content');
        navigator.clipboard.writeText(codeContent).then(function() {
        }, function() {
            alert('Failed to copy!');
        });
    });
</script>

<br>

<h2>
    This website is subsequent work to the following research:
</h2> 

<ol>
    <li>
        N. Raj Prabhu, B. Lay, S. Welker, N. Lehmann-Willenbrock and T. Gerkmann, 
        "EMOCONV-Diff: Diffusion-Based Speech Emotion Conversion for Non-Parallel and in-the-Wild Data," 
        <i>ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, 
        Seoul, Korea, Republic of, 2024, pp. 11651-11655, 
        <a href="https://doi.org/10.1109/ICASSP48485.2024.10447372" target="_blank">doi:10.1109/ICASSP48485.2024.10447372</a>.
    </li>
    <li>
        N. Raj Prabhu, N. Lehmann-Willenbrock and T. Gerkmann, 
        "In-the-wild Speech Emotion Conversion Using Disentangled Self-Supervised Representations and Neural Vocoder-based Resynthesis," 
        <i>Speech Communication; 15th ITG Conference</i>, Aachen, 2023, pp. 176-180, 
        <a href="https://doi.org/10.30420/456164034" target="_blank">doi:10.30420/456164034</a>.
    </li>
</ol>
